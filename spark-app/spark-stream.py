#!/usr/bin/env python3
from spark_crime import start_flux_crime
from spark_train import start_confirm_send_ia
from spark_select import handle_sql_topic
from pyspark.sql import SparkSession

# SparkSession obligatoire ici
spark = SparkSession.builder.appName("MasterStreamingJob").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

# Liste des couples topic entrÃ©e/sortie
topic_pairs = [
    # ("sql-queries-user1", "sql-results-user1"),
    # ("sql-queries-admin", "sql-results-admin"),
]

# Liste des StreamingQuery Ã  suivre
all_streams = []

# Lancer tous les flux SQL
for input_topic, output_topic in topic_pairs:
    print(f"ğŸš€ DÃ©marrage du flux SQL : {input_topic} â†’ {output_topic}")
    stream = handle_sql_topic(spark, input_topic, output_topic)
    all_streams.append(stream)

# Ajouter le flux d'accusÃ© de rÃ©ception IA
print("ğŸš€ DÃ©marrage du flux lecture du topic Kafka train-data")
train_confirm_stream = start_confirm_send_ia(spark)
all_streams.append(train_confirm_stream)

# Ajouter le flux d'insertion de crimes + publication IA
print("ğŸš€ DÃ©marrage du flux Kafka vers PostgreSQL + IA : send-data â†’ crimes/train-data")
crime_stream = start_flux_crime(spark)
all_streams.append(crime_stream)

# Afficher Ã©tat global
print(f"âœ… {len(all_streams)} flux lancÃ©s. En attente...")

# âœ… Tous les flux sont lancÃ©s
print("âœ… Tous les flux sont lancÃ©s. En attente globale...")

# ğŸ”„ Attente globale de tous les flux Spark
try:
    spark.streams.awaitAnyTermination()
except Exception as e:
    print(f"âŒ Un des flux sâ€™est terminÃ© avec une erreur : {e}")